{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4HI2mpwlrcn"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "679Lmwt3l1Bk"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSPCom-KmApV"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the code in this notebook comes from [tensorflow's GitHub](https://github.com/tensorflow/docs/tree/master/site/en/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- describe the types of layers that are distinctive for convolutional nets;\n",
    "- utilize `tensorflow` to build CNNs;\n",
    "- evaluate CNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are CNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network):\n",
    "\n",
    "- \"CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. However, CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns.\"\n",
    "<br/>\n",
    "<br/>\n",
    "- \"Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolving and Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two distinctive types of layer inside of a typical CNN (and there may be several of each in a single network) are **convolutional** and **pooling** layers. Let's look at each in turn.\n",
    "\n",
    "\n",
    "#### Convolution\n",
    "Convolutional nets employ [convolutions](https://en.wikipedia.org/wiki/Convolution), which are a certain kind of transformation. In the context of neural networks processing images, this can be thought of as sliding a filter (of weights) over the image matrix to produce a new matrix of values. (We'll detail the calculation below.) The relative smallness of the filter means both that there will be relatively few parameters to learn and that the values representing certain areas of the image will be affected only by the values of *nearby areas*. This helps the network in **feature detection**. Let's check out some visualizations [here](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/).\n",
    "\n",
    "Utkarsh Sinha shows us some examples of different kinds of filters [here](http://aishack.in/tutorials/convolutions/).\n",
    "\n",
    "Suppose we have a 3x3 image and a 2x2 filter. Then the calculation goes as follows:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & g \\\\\n",
    "h & j & k\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "f_1 & f_2 \\\\\n",
    "f_3 & f_4\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "f_1a + f_2b + f_3d + f_4e & f_1b + f_2c + f_3e + f_4g \\\\\n",
    "f_1d + f_2e + f_3h + f_4j & f_1e + f_2g + f_3j + f_4k\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "In words: Line up the filter with the image, multiply all the corresponding pairs and then add up those products. Repeat for all positions of the filter as allowed by [the stride and the padding](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/). The relative position of the filter to the image will tell you which entry in the resultant matrix you're filling in.\n",
    "\n",
    "##### Exercise\n",
    "Let's try an example of horizontal edge detection. One good filter for that might look like:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "10 & 10 & 10 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-10 & -10 & -10\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Suppose we apply this filter to (i.e. *convolve*) an image with a clear horizontal edge, such as this one:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<details><summary>\n",
    "    Answer here\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $\\begin{bmatrix}\n",
    "    0 & 0 & 0 \\\\\n",
    "    6000 & 6000 & 6000 \\\\\n",
    "    6000 & 6000 & 6000 \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}$\n",
    "    Notice how the edge is now \"highlighted\"!\n",
    "    </details>\n",
    "\n",
    "[Here](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) is another good resource.\n",
    "\n",
    "#### Pooling\n",
    "What is pooling? The main goal in inserting a pooling layer is to reduce dimensionality, which helps to reduce both network computation and model overfitting. This is generally a matter of reducing a matrix or tensor of values to  some smaller size, and the most common way of doing this is by partitioning the large matrix into $n$ x $n$ blocks and then replacing each with the largest value in the block. Hence we speak of \"MaxPooling\".\n",
    "\n",
    "Let's check out [this page](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/pooling_layer.html) for some visuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the TensorFlow Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7KBpffWzlxH"
   },
   "source": [
    "### Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRFxccghyMVo"
   },
   "source": [
    "### Download and prepare the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWoEqyMuXFF4"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWoEqyMuXFF4"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWoEqyMuXFF4"
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255, test_images / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oewp-wYg31t9"
   },
   "source": [
    "### Create the convolutional base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQvqXpNyN3x"
   },
   "source": [
    "The 6 lines of code below define the convolutional base using a common pattern: a stack of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers.\n",
    "\n",
    "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to color channels, MNIST has one (because the images are grayscale), whereas a color image has three (R,G,B). In this example, we will configure our CNN to process inputs of shape (28, 28, 1), which is the format of MNIST images. We do this by passing the argument `input_shape` to our first layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9YmGQBQPrdn"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvDVFkg-2DPm"
   },
   "source": [
    "Let display the architecture of our model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9 * 32 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18432"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9 * 64) * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9 * 64 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-C4XBg4UTJy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters depends on the number of input and output channels of the layer in question. For more, see [this post](https://medium.com/@zhang_yang/number-of-parameters-in-dense-and-convolutional-neural-networks-34b54c2ec349) and [this post](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_j-AXYeZ2GO5"
   },
   "source": [
    "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as we go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically,  as the width and height shrink, we can afford (computationally) to add more output channels in each Conv2D layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_v8sVOtG37bT"
   },
   "source": [
    "### Add Dense layers on top\n",
    "To complete our model, we will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, we will flatten (or unroll) the 3D output to 1D,  then add one or more Dense layers on top. MNIST has 10 output classes, so we use a final Dense layer with 10 outputs and a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRs95d6LUVEi"
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipGiQMcR4Gtq"
   },
   "source": [
    " Here's the complete architecture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Yu_m-TZUWGX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNKXi-Gy3RO-"
   },
   "source": [
    "As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*3*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3odqfHP4M67"
   },
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdDzI75PUXrG"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdDzI75PUXrG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 14:55:11.845598: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-01-17 14:55:12.216367: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 5s 81ms/step - loss: 0.9270 - accuracy: 0.7413 - val_loss: 0.3164 - val_accuracy: 0.9045\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 3s 75ms/step - loss: 0.2394 - accuracy: 0.9269 - val_loss: 0.1737 - val_accuracy: 0.9481\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 3s 73ms/step - loss: 0.1480 - accuracy: 0.9557 - val_loss: 0.1293 - val_accuracy: 0.9622\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 3s 72ms/step - loss: 0.1130 - accuracy: 0.9653 - val_loss: 0.0994 - val_accuracy: 0.9711\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 3s 73ms/step - loss: 0.0976 - accuracy: 0.9695 - val_loss: 0.1033 - val_accuracy: 0.9713\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 4s 78ms/step - loss: 0.0794 - accuracy: 0.9752 - val_loss: 0.0871 - val_accuracy: 0.9745\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 3s 75ms/step - loss: 0.0715 - accuracy: 0.9781 - val_loss: 0.0778 - val_accuracy: 0.9767\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 3s 73ms/step - loss: 0.0604 - accuracy: 0.9814 - val_loss: 0.0837 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 3s 74ms/step - loss: 0.0523 - accuracy: 0.9843 - val_loss: 0.0694 - val_accuracy: 0.9796\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 3s 73ms/step - loss: 0.0496 - accuracy: 0.9841 - val_loss: 0.0642 - val_accuracy: 0.9816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c8f8a6b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, validation_split=.25, epochs=10, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKgyC5K_4O0d"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtyDF0MKUcM7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0453 - accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LvwaKhtUdOo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9845000505447388\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cfJ8AR03gT5"
   },
   "source": [
    "As you can see, our simple CNN has achieved a really high test accuracy. Not bad for a few lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking a Particular Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.0065868e-08, 9.9979478e-01, 6.7001611e-06, 4.5734687e-08,\n",
       "        4.6582110e-05, 1.9461110e-07, 2.9743146e-06, 1.4609967e-04,\n",
       "        2.3512389e-06, 2.7716717e-07]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_images[2].reshape(1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[2].reshape(28, 28));"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "intro_to_cnns.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
