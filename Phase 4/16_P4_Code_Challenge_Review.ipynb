{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Phase 4 Code Challenge Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Pipelines and gridsearching\n",
    "- Ensemble Methods\n",
    "- Natural Language Processing\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.call import call_on_students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Pipelines and Gridsearching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the benefits of using a pipline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline →  it becomes an sklearn estimator →  fit, transform, cross validate, etc. \n",
    "- Compartmentalizes each part of the process\n",
    "- Flexible - can change each part separately\n",
    "- Helps prevent data leakage in cross validation\n",
    "- Column transformer: calling diff transformation objects on diff columns. Ohe, scaling, imputation only on the columns that need it. Like “mini” pipelines for each step\n",
    "- Handling Missing Values (nulls) can happen differently on diff columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a gridsearch achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with doing model tuning by hand - sometimes each adjustment would affect the other if you combined adjustments\n",
    "- Really you need to tune the model is tandem\n",
    "\n",
    "Grid Search:\n",
    "Automatically/Systematically searching for the optimal combination of hypterparametres by providing a grid to our Grid Search with the values of each hypterparameter that we want to try\n",
    "Grid search will evaluate all those combos with the evaluation metric that we choose, then return the best combination of hypterparameters = the best model\n",
    "scikit-Learn’s GridSearchCV class w/ .fit() method \n",
    "Cross validates each of those combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a pipeline with a scaler and a logistic regression model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1). Don't worry for now about a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                   ('lr', LogisticRegression(random_state=1, C=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test and then gridsearch over pipelines like the one you just built to find the best-performing model. Try C (inverse regularization) values of 10, 1, and 0.1. Try out the best estimator on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] lr__C=0.1 .......................................................\n",
      "[CV] ........................................ lr__C=0.1, total=   0.0s\n",
      "[CV] lr__C=0.1 .......................................................\n",
      "[CV] ........................................ lr__C=0.1, total=   0.0s\n",
      "[CV] lr__C=0.1 .......................................................\n",
      "[CV] ........................................ lr__C=0.1, total=   0.0s\n",
      "[CV] lr__C=0.1 .......................................................\n",
      "[CV] ........................................ lr__C=0.1, total=   0.0s\n",
      "[CV] lr__C=0.1 .......................................................\n",
      "[CV] ........................................ lr__C=0.1, total=   0.0s\n",
      "[CV] lr__C=1 .........................................................\n",
      "[CV] .......................................... lr__C=1, total=   0.0s\n",
      "[CV] lr__C=1 .........................................................\n",
      "[CV] .......................................... lr__C=1, total=   0.0s\n",
      "[CV] lr__C=1 .........................................................\n",
      "[CV] .......................................... lr__C=1, total=   0.0s\n",
      "[CV] lr__C=1 .........................................................\n",
      "[CV] .......................................... lr__C=1, total=   0.0s\n",
      "[CV] lr__C=1 .........................................................\n",
      "[CV] .......................................... lr__C=1, total=   0.0s\n",
      "[CV] lr__C=10 ........................................................\n",
      "[CV] ......................................... lr__C=10, total=   0.0s\n",
      "[CV] lr__C=10 ........................................................\n",
      "[CV] ......................................... lr__C=10, total=   0.0s\n",
      "[CV] lr__C=10 ........................................................\n",
      "[CV] ......................................... lr__C=10, total=   0.0s\n",
      "[CV] lr__C=10 ........................................................\n",
      "[CV] ......................................... lr__C=10, total=   0.0s\n",
      "[CV] lr__C=10 ........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................................... lr__C=10, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(C=1,\n",
       "                                                           random_state=1))]),\n",
       "             param_grid={'lr__C': [0.1, 1, 10]}, return_train_score=True,\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "grid = {\n",
    "    'lr__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, grid, verbose=2, return_train_score=True)\n",
    "#default cv=5\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('lr', LogisticRegression(C=0.1, random_state=1))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the best estimator out on test data\n",
    "grid_search.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What sorts of ensembling methods have we looked at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble: a model consisting of a bunch of other models\n",
    "Generalize (perform on unseen test data) well\n",
    "Decreases variance →  less overfitting\n",
    "More complexity: have to train each model or part of model\n",
    "BUT sometimes means →  less interpretation\n",
    "Ensembles consisting of a bunch of trees still have feature importances\n",
    "But ensembles consisting of trees and logistic regs and knn’s have feature imps, coeffs, and\tbut not those for the whole model\n",
    "Takse up more space: have to keep each model\n",
    "BUT also means →  more computational power and time\n",
    "We want differently randomized models that are overfit in different ways\n",
    "-Bagging: a bag of marbles, each marble is a model\n",
    "Many model types naturally overfit\n",
    "So, we can randomly assign data and features to each model\n",
    "Each model overfits in different ways\n",
    "Then we aggregate all the models \n",
    "Bootstrap (replacement) aggregating\n",
    "Repeat algorithm many times w/ replacement (bootstrap)\n",
    "Create multiple samples from your data\n",
    "Train models on those samples\n",
    "Final model predicts by averaging or voting across those models\n",
    "→  hope that that smooths over/cancels out the different ways each model overfits to reduce variance\n",
    "→  low variance since it averages out quirks the individual models might’ve learned\n",
    "4 Levels of Randomization\n",
    "1. Simple Bag: BaggingClassifier\n",
    "- Train each model on random samples w/ replacement\n",
    "- Bagging Algorithm\n",
    "Take a sample of your X_train and fit a decision tree to it.\n",
    "Replace the first batch of data and repeat.\n",
    "When you've got as many trees as you like, make use of all your individual trees' predictions to come up with some holistic prediction.\n",
    "(Most obviously, we could take the average of our predictions, but there are other methods we might try.)\n",
    "Bagging:\n",
    "Because we're resampling our data with replacement, we're bootstrapping.\n",
    "Because we're making use of our many samples' predictions, we're aggregating.\n",
    "Because we're bootstrapping and aggregating all in the same algorithm, we're bagging.\n",
    "\n",
    "2. Random Forest: random sample AND give model all the features, but at each node only allow it to choose from a random subset of those features to split on, using a splitting criteria like gini or entropy to split on\n",
    "Each tree in the forest is very different\n",
    "Finds best split based on gini or entropy, like normal decision trees\n",
    "3. Extra Trees: random sample, AND random subset of features, AND among that random subset of features, but not splitting based on gini or entropy, just choosing a random feature to split on \n",
    "\n",
    "Ensembles of Different Model Types: \n",
    "Maybe you have some linear data, some non-linear, a lot of categorical data\n",
    "Logreg models better for linear relationships, but trees are better for the last two\n",
    "2 methods: \n",
    "VotingClassifier or VotingRegressor: taking a simple average (or a vote) of the outputs of the sub-models\n",
    "Sometimes uses weighted averaging\n",
    "StackingClassifier or StackingRegressor: feeding the ouputted predictions of a bunch of sub-models as features in another final model\n",
    "Also known as Meta-Classifier/Meta-Regressor\n",
    "Approaching a neural network \n",
    "Estimates the weights in the averaging for us\n",
    "This means we'll have one layer of base estimators and another layer that is \"trained to optimally combine the model predictions to form a new set of predictions\". See this short blog post for more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is random about a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Still bootstrapping: random samples w/ replacement\n",
    "- Now give each tree all the features, but at each node we give the tree a random subset of the features to choose from/split on \n",
    "\n",
    "Pros:\n",
    "\n",
    "- High performance, low variance\n",
    "- Transparent: inherited from Decision Tree (white box??)\n",
    "\n",
    "Cons:\n",
    "\n",
    "- So many trees to plant\n",
    "- Computationally expensive\n",
    "- Memory: every tree stored back to memory, almost as memory intensive as KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What hyperparameters of a random forest might it be useful to tune? How so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_estimators: #s of sub-models to train\n",
    "- Max_features: feature subsetting\n",
    "    - Can also set this parameter in a Bagging Classifier and it will make that bagging ~ Random Forest\n",
    "- Bootstrap: whether you want to random sample the dataset or not. We do want this in order to grow different trees each time\n",
    "- Max_samples: size of samples\n",
    "    - Must be 0 -1\n",
    "    - Default = None, will sample the length of X\n",
    "    - If it is = 1: →  \n",
    "    - This can be the same size as the full datase t b/c we allow replacement after we take each data point\n",
    "    - So we can have a bunch of diff samples that are all the same size as the original for each model\n",
    "- also all same parameters as a decision tree: max depth, min samples leaf, min samples split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a random forest model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440559440559441"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "rfc = RandomForestClassifier(random_state=1)\n",
    "rfc.fit(X_train, y_train)\n",
    "# y_pred = rfc.predict(X_test)\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# scores = cross_val_score(estimator=rfc, X=X_train, y=y_train, cv=5)\n",
    "# scores\n",
    "\n",
    "# rfc.score(X_train, y_train)\n",
    "# rfc.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 23
   },
   "source": [
    "# 3) Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Concepts\n",
    "\n",
    "### Some Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 24
   },
   "outputs": [],
   "source": [
    "# Each sentence is a document\n",
    "sentence_one = \"Harry Potter is the best young adult book about wizards\"\n",
    "sentence_two = \"Um, EXCUSE ME! Ever heard of Earth Sea?\"\n",
    "sentence_three = \"I only like to read non-fiction.  It makes me a better person.\"\n",
    "\n",
    "# The corpus is composed of all of the documents\n",
    "corpus = [sentence_one, sentence_two, sentence_three]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Pre-processing\n",
    "\n",
    "List at least three steps you can take to turn raw text like this into something that would be semantically valuable (aka ready to turn into numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove stop words like \"is\" that have no semantic value\n",
    "2. Lower case all the words\n",
    "3. Remove punctuation\n",
    "4. Stemming or lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what vectorized text would look like as a dataframe.\n",
    "\n",
    "If you vectorize the above corpus, what would the rows and columns be in the resulting dataframe (aka document term matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represented as a document term matrix:\n",
    "Each row is a document\n",
    "Each column is a unique vocabulary n-gram (a token). 1 or 0 for if that doc has it\n",
    "So each row is a numerical vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does TF-IDF do?\n",
    "\n",
    "Also, what does TF-IDF stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 33
   },
   "source": [
    "## NLP in Code\n",
    "\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 34
   },
   "outputs": [],
   "source": [
    "# New section, new data\n",
    "policies = pd.read_csv('data/2020_policies_feb_24.csv')\n",
    "\n",
    "def warren_not_warren(label):\n",
    "    \n",
    "    '''Make label a binary between Elizabeth Warren\n",
    "    speeches and speeches from all other candidates'''\n",
    "    \n",
    "    if label =='warren':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "policies['candidate'] = policies['candidate'].apply(warren_not_warren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 35
   },
   "source": [
    "The dataframe loaded above consists of policies of 2020 Democratic presidential hopefuls. The `policy` column holds text describing the policies themselves.  The `candidate` column indicates whether it was or was not an Elizabeth Warren policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 36
   },
   "outputs": [],
   "source": [
    "policies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 37
   },
   "source": [
    "The documents for activity are in the `policy` column, and the target is candidate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Relevant Class, Then Instantiate and Fit a Count Vectorizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First! Train-test split the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Your Text, Then Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 42
   },
   "outputs": [],
   "source": [
    "# Code here to transform train and test sets with the vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 44
   },
   "outputs": [],
   "source": [
    "# Code here to instantiate and fit a Random Forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here to evaluate your model on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "# 4) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "## Clustering Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Describe how the K-Means algorithm updates its cluster centers (centroids) after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### What is inertia, and how does K-Means use inertia to determine the best estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Please also describe the method you can use to evaluate clustering using inertia.\n",
    "\n",
    "Documentation, for reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### What other metric do we have to score the clusters which are formed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Describe the difference between it and inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "## Clustering in Code with Heirarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "After the above conceptual review of KMeans, let's practice coding with agglomerative clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# New dataset for this section!\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Prepare our Data for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "What steps do we need to take to preprocess our data effectively?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Code to preprocess the data\n",
    "\n",
    "# Name the processed data X_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Import the Relevant Class, Then Instantiate and Fit a Hierarchical Agglomerative Clustering Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Let's use `n_clusters = 2` to start (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Import the relevent clustering algorithm\n",
    "\n",
    "\n",
    "# Instantiate and fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Calculate a silhouette score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Write a Function to Test Different Options for `n_clusters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "The function should take in the number for `n_clusters` and the data to cluster, fit a new clustering model using that parameter to the data, print the silhouette score, then return the labels attribute from the fit clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
